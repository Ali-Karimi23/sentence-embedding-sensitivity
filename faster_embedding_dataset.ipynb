{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk , Dataset, DatasetDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf463d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e02c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO = \"/content/drive/MyDrive/00-github/sentence-embedding-sensitivity\"\n",
    "DATA = os.path.join(REPO,\"Data\")\n",
    "DATASETS = os.path.join(DATA,\"datasets\")\n",
    "SICK_DATA = os.path.join(DATA,\"sick_dataset\")\n",
    "SR_DATA = os.path.join(DATA,\"sr_dataset\")\n",
    "VISLA_DATA = os.path.join(DATA,\"VISLA\")\n",
    "\n",
    "model_dict = {\n",
    "    \"par-dis-roberta\": \"paraphrase-distilroberta-base-v1\",\n",
    "    \"roberta-base-v3\": \"msmarco-roberta-base-v3\",\n",
    "    \"par-mpnet\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"par-xlm-r\": \"paraphrase-xlm-r-multilingual-v1\",\n",
    "    \"labse\": \"LaBSE\",\n",
    "    \"e5-base\": \"intfloat/e5-base-v2\",\n",
    "    \"gte-base\": \"thenlper/gte-base\",\n",
    "    \"bge-base-v15\": \"BAAI/bge-base-en-v1.5\"\n",
    "}\n",
    "\n",
    "# \"MRPC\": (\"glue\", \"mrpc\"),\n",
    "\n",
    "benchmark_datasets = {\n",
    "    \"MRPC\": (\"glue\", \"mrpc\"),\n",
    "    \"QQP\": (\"glue\", \"qqp\"),\n",
    "    \"PAWS\": (\"paws\", \"labeled_final\"),\n",
    "    \"STS\": (\"glue\", \"stsb\"),\n",
    "    \"SICK\": f\"{SICK_DATA}\",\n",
    "    \"SR\": f\"{SR_DATA}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52817ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title  Fast GPU modes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    ENCODE_BS = 1024 if \"A100\" in gpu_name else 256\n",
    "    AMP_DTYPE = torch.bfloat16 if \"A100\" in gpu_name else torch.float16\n",
    "else:\n",
    "    gpu_name = \"CPU\"\n",
    "    ENCODE_BS = 64\n",
    "    AMP_DTYPE = None\n",
    "\n",
    "print(f\"Running on {gpu_name}, batch_size={ENCODE_BS}, dtype={AMP_DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset directory if it doesn't exist\n",
    "os.makedirs(DATASETS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val = {\n",
    "    \"MRPC\" : \"test\",\n",
    "    \"QQP\" : \"validation\",\n",
    "    \"PAWS\" : \"test\",\n",
    "    \"STS\" : \"validation\",\n",
    "    \"SICK\" : \"test\",\n",
    "    \"SR\" : \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cf0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title  Main loop to embed datasets\n",
    "\n",
    "for dataset_name, dataset_id in benchmark_datasets.items():\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    t1 = time.time()\n",
    "    # Load dataset\n",
    "    if isinstance(dataset_id, tuple):\n",
    "        dataset = load_dataset(*dataset_id)\n",
    "        t2 = time.time()\n",
    "        print(f\"Loaded {dataset_name} from HuggingFace in {t2-t1:.1f} seconds\")\n",
    "    else:\n",
    "        dataset = load_from_disk(dataset_id)\n",
    "        t2 = time.time()\n",
    "        print(f\"Loaded {dataset_name} from disk in {t2-t1:.1f} seconds\")\n",
    "\n",
    "    # Load each model and encode\n",
    "    for model_name, model_id in model_dict.items():\n",
    "        print(f\"--- Model: {model_name} ---\")    \n",
    "        \n",
    "        # Load model\n",
    "        t1 = time.time()\n",
    "        model = SentenceTransformer(model_id, device=device)\n",
    "        print(type(model.tokenizer))\n",
    "        t2 = time.time()\n",
    "        print(f\"Loaded model in {t2-t1:.1f} seconds\")\n",
    "\n",
    "        # Move to GPU in lower precision if possible\n",
    "        t1 = time.time()\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                base = model._first_module().auto_model\n",
    "                base.to(dtype=AMP_DTYPE, device=device)\n",
    "            except Exception:\n",
    "                pass\n",
    "        t2 = time.time()\n",
    "        print(f\"Moved model to GPU in {t2-t1:.1f} seconds\")\n",
    "        \n",
    "        # Encode each split\n",
    "        dataset_dict ={}\n",
    "        for split in [\"train\",test_val[dataset_name]]:\n",
    "\n",
    "            # Choose text columns\n",
    "            if all(c in dataset[split].column_names for c in (\"sentence1\", \"sentence2\")):\n",
    "                colA, colB = \"sentence1\", \"sentence2\"\n",
    "            else:\n",
    "                colA, colB = \"question1\", \"question2\"\n",
    "            print(f\"Processing split: {split}, columns: {colA}, {colB}\")\n",
    "            \n",
    "\n",
    "            # Pull texts\n",
    "            t1 = time.time()\n",
    "            texts_a = dataset[split][colA]\n",
    "            texts_b = dataset[split][colB]\n",
    "            t2 = time.time()\n",
    "            print(f\"Pulled {len(texts_a)} texts from dataset in {t2-t1:.1f} seconds\")\n",
    "            \n",
    "            # Encode both sides at once\n",
    "            with torch.inference_mode():\n",
    "                with torch.autocast(\"cuda\", dtype=AMP_DTYPE, enabled=(device==\"cuda\")):\n",
    "                    t1 = time.time()\n",
    "                    embA = model.encode(\n",
    "                        texts_a,\n",
    "                        batch_size=ENCODE_BS,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=False,\n",
    "                        show_progress_bar=True,\n",
    "                    )\n",
    "                    t2 = time.time()\n",
    "                    print(f\"Encoded side A in {t2-t1:.1f} seconds\")\n",
    "                    t1 = time.time()\n",
    "                    embB = model.encode(\n",
    "                        texts_b,\n",
    "                        batch_size=ENCODE_BS,\n",
    "                        convert_to_numpy=True,\n",
    "                        normalize_embeddings=False,\n",
    "                        show_progress_bar=True,\n",
    "                    )\n",
    "                    t2 = time.time()\n",
    "                    print(f\"Encoded side B in {t2-t1:.1f} seconds\")\n",
    "\n",
    "                    # Calculating Cosine similarity\n",
    "                    t1 = time.time()\n",
    "                    embA_norm = embA / np.linalg.norm(embA, axis=1, keepdims=True)\n",
    "                    embB_norm = embB / np.linalg.norm(embB, axis=1, keepdims=True)\n",
    "                    cosine_scores = np.sum(embA_norm * embB_norm, axis=1)\n",
    "                    t2 = time.time()\n",
    "                    print(f\"Calculated cosine similarity in {t2-t1:.1f} seconds\")\n",
    "                    print(f\"Cosine scores: min {cosine_scores.min():.4f}, max {cosine_scores.max():.4f}, mean {cosine_scores.mean():.4f}, std {cosine_scores.std():.4f}\")\n",
    "            if split == \"train\":\n",
    "                save_path = os.path.join(DATASETS, f\"{dataset_name}_{model_name}_train.npz\")\n",
    "            else:\n",
    "                save_path = os.path.join(DATASETS, f\"{dataset_name}_{model_name}_test.npz\")\n",
    "            np.savez_compressed(\n",
    "                save_path,\n",
    "                embedding1=embA.astype(np.float16),\n",
    "                embedding2=embB.astype(np.float16),\n",
    "                cosine_scores=cosine_scores.astype(np.float16),\n",
    "                label = np.array(dataset[split][\"label\"],dtype=np.int32)\n",
    "            )\n",
    "\n",
    "            # Save to disk\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "        # Clean up\n",
    "        del model\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
