{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd75461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset\n",
    "dataset_type = \"Binary-dataset\" # @param [\"Binary-dataset\", \"Multi-class-dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Speedup on A100\n",
    "torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "# ========= settings =========\n",
    "DATA_DIR = \"/content/drive/MyDrive/00-github/sentence-embedding-sensitivity/Data\"\n",
    "DATA_DIR = os.path.join(DATA_DIR, dataset_type)\n",
    "OUT_DIR  = \"/content/drive/MyDrive/00-github/sentence-embedding-sensitivity/Results\"        \n",
    "EPOCHS     = 100\n",
    "BATCH_SIZE = 512\n",
    "LR         = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "P_DROP     = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available:\", torch.cuda.get_device_name(0))\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        print(\"MPS is available\")\n",
    "        return torch.device(\"mps\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# pattern: <dataset>_<model>_(train|test).npz\n",
    "def parse_name(fname):\n",
    "    base = os.path.basename(fname)[:-4]  # remove .npz\n",
    "    prefix, split = base.rsplit(\"_\", 1)\n",
    "    dataset, model = prefix.split(\"_\", 1)\n",
    "    assert split in (\"train\", \"test\")\n",
    "    return dataset, model, split\n",
    "\n",
    "# Dataset class\n",
    "class PairNPZ(Dataset):\n",
    "    def __init__(self, path):\n",
    "        d = np.load(path)\n",
    "        self.e1 = d[\"embedding1\"].astype(np.float32)\n",
    "        self.e2 = d[\"embedding2\"].astype(np.float32)\n",
    "        self.y  = d[\"label\"].astype(np.int64)\n",
    "        assert self.e1.shape == self.e2.shape\n",
    "        assert len(self.e1) == len(self.y)\n",
    "        self.dim = self.e1.shape[1]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.from_numpy(self.e1[i]),\n",
    "            torch.from_numpy(self.e2[i]),\n",
    "            torch.tensor(self.y[i], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "# Model class\n",
    "class PairMLP(nn.Module):\n",
    "    def __init__(self, dim=768, p_drop=0.3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.out = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, e1, e2):\n",
    "        x = torch.cat([e1, e2, torch.abs(e1-e2), e1*e2], dim=-1)\n",
    "        x = self.drop(self.relu(self.fc1(x)))\n",
    "        x = self.drop(self.relu(self.fc2(x)))\n",
    "        return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Evaluation function\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for e1, e2, y in loader:\n",
    "        e1, e2 = e1.to(device), e2.to(device)\n",
    "        logits = model(e1, e2)\n",
    "        pred = logits.argmax(1).cpu().numpy()\n",
    "        ys.append(y.numpy()); ps.append(pred)\n",
    "\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    if num_classes == 2:\n",
    "        f1_one  = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        f1_zero = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    else:\n",
    "        f1_one = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_zero = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    return acc, f1_one, f1_zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training function\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_one(model, loader, optim, crit, device):\n",
    "    model.train()\n",
    "    ys, ps = [], []\n",
    "    total = 0.0\n",
    "    for e1, e2, y in loader:\n",
    "        e1, e2, y = e1.to(device), e2.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(e1, e2)\n",
    "            loss = crit(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        total += loss.item() * y.size(0)\n",
    "        ps.append(logits.argmax(1).detach().cpu().numpy())\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return total/len(y_true), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Pair function\n",
    "def run_pair(train_path, test_path, dataset, model_name, device):\n",
    "    train_ds = PairNPZ(train_path)\n",
    "    test_ds  = PairNPZ(test_path)\n",
    "    dim = train_ds.dim\n",
    "\n",
    "    pin = device.type == \"cuda\"\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin)\n",
    "    if dataset in [\"MRPC\",\"QQP\",\"PAWS\"]:\n",
    "        model = PairMLP(dim=dim, p_drop=P_DROP).to(device)\n",
    "    else:\n",
    "        model = PairMLP(dim=dim, p_drop=P_DROP, num_classes=5).to(device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    crit  = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_acc_hist, test_acc_hist = [], []\n",
    "    best = {\"acc\": -1.0, \"f1_one\": 0.0, \"f1_zero\": 0.0, \"epoch\": -1}\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        _, tr_acc = train_one(model, train_loader, optim, crit, device)\n",
    "        te_acc, f1_one, f1_zero = eval_epoch(model, test_loader, device)\n",
    "        train_acc_hist.append(tr_acc); test_acc_hist.append(te_acc)\n",
    "        if te_acc > best[\"acc\"]:\n",
    "            best = {\"acc\": float(te_acc), \"f1_one\": float(f1_one), \"f1_zero\": float(f1_zero), \"epoch\": epoch}\n",
    "        print(f\"[{dataset} | {model_name}] epoch {epoch}: train_acc={tr_acc:.3f}  test_acc={te_acc:.3f}\")\n",
    "\n",
    "    os.makedirs(os.path.join(OUT_DIR, \"images\"), exist_ok=True)\n",
    "    img_path = os.path.join(OUT_DIR, \"images\", f\"{dataset}_{model_name}_acc.png\")\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, EPOCHS+1), train_acc_hist, label=\"train acc\")\n",
    "    plt.plot(range(1, EPOCHS+1), test_acc_hist,  label=\"test acc\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.tight_layout()\n",
    "    plt.title(f\"{dataset} / {model_name} (best@{best['epoch']}: {best['acc']:.3f})\")\n",
    "    plt.savefig(img_path, dpi=160); plt.close()\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model_name,\n",
    "        \"acc\": best[\"acc\"],\n",
    "        \"f1_one\": best[\"f1_one\"],\n",
    "        \"f1_zero\": best[\"f1_zero\"],\n",
    "        \"plot_path\": img_path,\n",
    "    }\n",
    "\n",
    "def collect_pairs(folder):\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(\".npz\")]\n",
    "    buckets = {}\n",
    "    for f in files:\n",
    "        dset, model, split = parse_name(f)\n",
    "        key = (dset, model)\n",
    "        buckets.setdefault(key, {})[split] = os.path.join(folder, f)\n",
    "    return {k: v for k, v in buckets.items() if \"train\" in v and \"test\" in v}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334734fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training and Evaluation\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "pairs = collect_pairs(DATA_DIR)\n",
    "results = []\n",
    "for (dset, model), splits in sorted(pairs.items()):\n",
    "    print(f\"\\n============= Running {dset} | {model} =============\")\n",
    "    row = run_pair(splits[\"train\"], splits[\"test\"], dset, model, device)\n",
    "    results.append(row)\n",
    "df = pd.DataFrame(results, columns=[\"dataset\",\"model\",\"acc\",\"f1_one\",\"f1_zero\",\"plot_path\"])\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(df)\n",
    "csv_path = os.path.join(OUT_DIR, \"results.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    old_df = pd.read_csv(csv_path)\n",
    "    df = pd.concat([old_df, df], ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
